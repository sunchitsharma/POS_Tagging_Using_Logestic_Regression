{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation and Formatting of Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we take data and spllit it acording to the tags, Now in case of multiple tags we only consider thetag that is given first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(train_data):\n",
    "    all_sentences = []\n",
    "    for line in train_data:\n",
    "        sentence = []\n",
    "        for tup in line:\n",
    "            word=tup[0].lower()\n",
    "            tag=tup[1].lower()\n",
    "            tag=tag.split('+')[0]\n",
    "            tag=tag.split('-')[0]\n",
    "            sentence.append((word, tag))\n",
    "        all_sentences.append(sentence)\n",
    "\n",
    "    return all_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As no features were given we took features that could be found in the text. These features were : Is it the first or last word, or was it fully or partially capitalized or was it a number or was it a prefix or a suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature(token, token_index, sent,vocab_idx):\n",
    "    token_feature = {    \n",
    "        'token'             : token,                                    # Token itself\n",
    "        'is_first'          : token_index == 0,                         # Is token at the beginning of the sentence\n",
    "        'is_last'           : token_index == len(sent)-1,               # Is token at the end of the sentence\n",
    "        'is_capitalized'    : token[0].upper() == token[0],             # Is first letter of token a capital letter\n",
    "        'is_all_capitalized': token.upper() == token,                   # Are all letters of token capital letters\n",
    "        'is_capitals_inside': token[1:].lower() != token[1:],           # Is there any capital letters in the token\n",
    "        'is_numeric'        : token.isdigit(),                          # Is there any digits in the token\n",
    "        'prefix-1'          : token[0],                                 # Token prefix containing only one letter\n",
    "        'prefix-2'          : '' if len(token) < 2  else token[:1],     # Token prefix containing two letters\n",
    "        'suffix-1'          : token[-1],                                # Token suffix containing only one letter\n",
    "        'suffix-2'          : '' if len(token) < 2  else token[-2:],    # Token suffix containing two letters\n",
    "        'prev-token'        : '' if token_index == 0     else sent[token_index - 1][0],     # Previous token in the sentence\n",
    "        '2-prev-token'      : '' if token_index <= 1     else sent[token_index - 2][0],     # Two previous token in the sentence\n",
    "        'next-token'        : '' if token_index == len(sent) - 1     else sent[token_index + 1][0],     # Next token in the sentence\n",
    "        '2-next-token'      : '' if token_index >= len(sent) - 2     else sent[token_index + 2][0]      # Two next token in the sentence\n",
    "        }\n",
    "    \n",
    "    if token not in vocab.keys():       # Add token to vocabulary\n",
    "        vocab[token] = vocab_idx\n",
    "        vocab_idx += 1\n",
    "    \n",
    "    # Adding features for a token \n",
    "    \n",
    "    \n",
    "    if token_feature['prefix-1'] not in vocab.keys():\n",
    "        vocab[token_feature['prefix-1']] = vocab_idx\n",
    "        vocab_idx += 1\n",
    "\n",
    "    if token_feature['prefix-2'] not in vocab.keys():\n",
    "        vocab[token_feature['prefix-2']] = vocab_idx\n",
    "        vocab_idx += 1\n",
    "\n",
    "    if token_feature['suffix-1'] not in vocab.keys():\n",
    "        vocab[token_feature['suffix-2']] = vocab_idx\n",
    "        vocab_idx += 1\n",
    "\n",
    "    if token_feature['suffix-2'] not in vocab.keys():\n",
    "        vocab[token_feature['suffix-2']] = vocab_idx\n",
    "        vocab_idx += 1\n",
    "\n",
    "    if token_feature['prev-token'] not in vocab.keys():\n",
    "        vocab[token_feature['prev-token']] = vocab_idx\n",
    "        vocab_idx += 1\n",
    "\n",
    "    if token_feature['2-prev-token'] not in vocab.keys():\n",
    "        vocab[token_feature['2-prev-token']] = vocab_idx\n",
    "        vocab_idx += 1\n",
    "\n",
    "    if token_feature['next-token'] not in vocab.keys():\n",
    "        vocab[token_feature['next-token']] = vocab_idx\n",
    "        vocab_idx += 1\n",
    "\n",
    "    if token_feature['2-next-token'] not in vocab.keys():\n",
    "        vocab[token_feature['2-next-token']] = vocab_idx\n",
    "        vocab_idx += 1\n",
    "\n",
    "    return token_feature, vocab_idx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making X and Y for logestic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to make features and poition labels for the logestic regression ie the X and Y axis for it. Therefore for all words we extract features. We also need to transform the features to make it fit for logestic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_data(all_sentences):\n",
    "    features   = []                                                     # X\n",
    "    pos_labels = []                                                     # Y\n",
    "    vocab_idx = 0\n",
    "    \n",
    "    for sent in all_sentences:\n",
    "        for token_index, token_pair in enumerate(sent):                 # Pick the index and token together \n",
    "            token = token_pair[0]                                       \n",
    "            f,vocab_idx=get_feature(token, token_index, sent,vocab_idx)# Extract features from token and append it to features list (x_train)\n",
    "            features.append(f)\n",
    "            pos_label = token_pair[1]                                   \n",
    "            pos_labels.append(pos_label)                                # Append pos label to pos_labels list (y_train) \n",
    "    \n",
    "    x_train = transform(features,vocab_idx) # Convert the X to vector form\n",
    "\n",
    "    \n",
    "    x_train=np.array(x_train)\n",
    "    return x_train, pos_labels "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tranformation of the features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the X is in form of a dictionary with true and false values and some suffix prefix and other data thus we need to convert it into a vector form to be understood by logestic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(x_train,vocab_idx):\n",
    "    X = []\n",
    "    for feature in x_train:\n",
    "        x_features = []\n",
    "        for key in feature:\n",
    "            if feature[key] is True:\n",
    "                x_features.append(1)\n",
    "            elif feature[key] is False:\n",
    "                x_features.append(0)\n",
    "            else:\n",
    "                if feature[key] not in vocab:\n",
    "                    x_features.append(vocab_idx)\n",
    "                    vocab_idx += 1\n",
    "                else:\n",
    "                    x_features.append(vocab[feature[key]])\n",
    "\n",
    "        X.append(x_features)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic logestic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, num_iter=10, fit_intercept=True, verbose=False):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iter = num_iter\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.verbose=verbose\n",
    "    \n",
    "    def __add_intercept(self, X):\n",
    "        intercept = np.ones((X.shape[0], 1))\n",
    "        return np.hstack((intercept, X))\n",
    "        \n",
    "        \n",
    "    def __sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def __loss(self, h, y):\n",
    "        return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        if self.fit_intercept:\n",
    "            X = self.__add_intercept(X)\n",
    "        \n",
    "        # weights initialization\n",
    "        self.w = np.zeros(X.shape[1])\n",
    "        \n",
    "        for i in range(self.num_iter):\n",
    "            z = np.dot(X, self.w)\n",
    "            h = self.__sigmoid(z)\n",
    "            gradient = np.dot(X.T, (h - y)) / len(y)\n",
    "            self.w -= self.learning_rate * gradient\n",
    "            \n",
    "            if(self.verbose == True and i % 10 == 0):\n",
    "                z = np.dot(X, self.w)\n",
    "                h = self.__sigmoid(z)\n",
    "        return self.w\n",
    "    \n",
    "    def predict_prob(self,X,wt):\n",
    "        if self.fit_intercept:\n",
    "            X = self.__add_intercept(X)\n",
    "    \n",
    "        return self.__sigmoid(np.dot(X,wt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting prediction and actual labels and testing how many of them are right and how many are wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(y_test, pred):\n",
    "    score=0\n",
    "    for i in range(0,len(y_test)):\n",
    "        if y_test[i]==pred[i]:\n",
    "            score+=1\n",
    "    \n",
    "    return score*100.0/len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Driver Code  : Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:15: RuntimeWarning: overflow encountered in exp\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown \n",
    "length = len(brown.tagged_sents())\n",
    "\n",
    "train_data = brown.tagged_sents()[:1000]            # Training Data\n",
    "all_sentences = read_data(train_data)               # Read all sentences\n",
    "x_train,y_train = form_data(all_sentences)          # making X and Y for logestic regression\n",
    "\n",
    "classes= list(set(y_train))                         # Getting classes for making models\n",
    "\n",
    "W=[]                                                # Weight list for all tags\n",
    "\n",
    "model = LogisticRegression()                        # Model : Logestic Regression\n",
    "\n",
    "for c in classes:                                   # One vs All : All classes have a model fitted with all other 0\n",
    "    y_label=[]\n",
    "    for y in y_train:\n",
    "        if y == c:\n",
    "            y_label.append(1)     # Current\n",
    "        else:\n",
    "            y_label.append(0)     # All Others\n",
    "            \n",
    "    weight=model.fit(x_train, y_label)\n",
    "    W.append(weight)              # All class models here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Driver Code : Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  10.3852263701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:15: RuntimeWarning: overflow encountered in exp\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "test_data = brown.tagged_sents()[-200:]            # Data for testing\n",
    "all_sentence = read_data(test_data)                # Fetching sentences\n",
    "x_test, y_test = form_data(all_sentence)           # Making X and Y\n",
    "\n",
    "result=[]\n",
    "for i in range(0,len(classes)):\n",
    "    p = model.predict_prob(x_test,W[i])            # Testing and fetching prediction\n",
    "    result.append(p)\n",
    "\n",
    "result = np.array(result).T\n",
    "\n",
    "pred=[]\n",
    "for r in result:\n",
    "    index=np.argmax(r,axis=0)                     # As it is one vs all : Argmax is the actual prediction\n",
    "    pred.append(classes[index])\n",
    "\n",
    "# Measure accuracy\n",
    "accuracy   = calc_accuracy(y_test, pred)         # Test with actual data\n",
    "print \"Accuracy : \",accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
